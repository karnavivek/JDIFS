{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries before starting RNN:<br>\n",
    "\n",
    "Question:  Why don't we us simple NN?, what is the need for RNN?<br>\n",
    "Answer:    Simple Neural Network lack the ability to share features learned across different positions of the text, i.e. it doesn't know which word to go where! Another reason is that our inputs will have very LARGE number of inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Numpy Package for Matrix Multiplications\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going implement a Basic RNN where, the Length of Inputs = Length of Outputs ($T_x = T_y$)\n",
    "\n",
    "<img src=\"/Users/karnavivek/Downloads/JDIFS/JDIFS/RNN/images/rnn1.jpeg\" width=\"600\" height=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    1/(1 + np.exp^(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario: Let's say we are building an RNN to predict the next word in a sentence. Our vocabulary is:<br>\n",
    "\n",
    "VOCAB - \"Hello world how are you doing I am well thank you this is a tutorial for learning recurrent neural network and hence we are going to learn this very nicely\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform calculations for one time step ($t$) then we will loop over total number of time steps ($T_x$), in order to process all the inputs, one at a time. <br>\n",
    "\n",
    "- $n_x$ - Number of units in a single time step of a single training example\n",
    "- $m$ - Number of Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'world',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'I',\n",
       " 'am',\n",
       " 'well',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'tutorial',\n",
       " 'for',\n",
       " 'learning',\n",
       " 'recurrent',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'and',\n",
       " 'hence',\n",
       " 'we',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'this',\n",
       " 'very',\n",
       " 'nicely']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB = \"Hello world how are you doing I am well thank you this is a tutorial for learning recurrent neural network and hence we are going to learn this very nicely\"\n",
    "\n",
    "#let's make it in format which is understandable by our computer\n",
    "VOCAB = VOCAB.split()\n",
    "VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 0,\n",
       " 'I': 1,\n",
       " 'a': 2,\n",
       " 'am': 3,\n",
       " 'and': 4,\n",
       " 'are': 5,\n",
       " 'doing': 6,\n",
       " 'for': 7,\n",
       " 'going': 8,\n",
       " 'hence': 9,\n",
       " 'how': 10,\n",
       " 'is': 11,\n",
       " 'learn': 12,\n",
       " 'learning': 13,\n",
       " 'network': 14,\n",
       " 'neural': 15,\n",
       " 'nicely': 16,\n",
       " 'recurrent': 17,\n",
       " 'thank': 18,\n",
       " 'this': 19,\n",
       " 'to': 20,\n",
       " 'tutorial': 21,\n",
       " 'very': 22,\n",
       " 'we': 23,\n",
       " 'well': 24,\n",
       " 'world': 25,\n",
       " 'you': 26}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {word: index for index, word in enumerate(sorted(set(VOCAB)))}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size\n",
    "\n",
    "#we will make an input sequence like \n",
    "sequence = [vocab[\"Hello\"], vocab[\"world\"], vocab[\"how\"], vocab[\"are\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Let's understand the shape of all the inputs'''\n",
    "\n",
    "xi = (n_x, m)\n",
    "a0 = (n_a, m) \n",
    "\n",
    "parameters = {\"Wax\": (n_a, n_x), \n",
    "              \"Waa\": (n_a, n_a), \n",
    "              \"Wya\": (n_y, n_a), \n",
    "              \"ba\": (n_a, 1), \n",
    "              \"by\": (n_y, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs = 2 (x1, a0)\n",
    "Outputs = 2 (y1, a1)\n",
    "'''\n",
    "\n",
    "xi = 'ohe 1D matrix' #x input\n",
    "a0 = np.zeros(xi.shape) #\n",
    "\n",
    "def one_step_RNN(xi, a0, parameters):\n",
    "\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    a1 = np.tanh(np.dot(Waa, a0) + np.dot(Wax, xi) + ba) #or we can use relu no linearity\n",
    "    y1 = sigmoid(np.dot(Wya, a1) + by)\n",
    "\n",
    "    cache = (a1, a0, xi, parameters) #we would need this for backpropogation\n",
    "    return a1, y1, cache\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
